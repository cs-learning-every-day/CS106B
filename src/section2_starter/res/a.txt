Recently, I built a predictive model at Google that will soon produce tremendous value, but I’m not a Data Scientist, nor do I have any formal training in Data Science. This got me thinking; why does so much industry and education exist around such specialized data science? Back in 2012, the Harvard Business Review publish an article proclaiming Data Scientist the sexiest job of the 21st century. Since then, thousands of jobs were created in the data science field to apply sophisticated statistical models to business problems. For the first time in history, it had become practical to apply deep maths to common problems. Many companies and entire industries were founded upon the practice of data science.
This article was ahead of its time in more ways than one. The article asserts that other skills common to data scientists should be curiosity, the ability to write code, visualize data, analyze data, and communicate effectively. The article quotes Hal Varian, the chief economist at Google, as saying “The sexy job in the next 10 years will be statisticians” — but there’s a stark disconnect between data analysts and statisticians, despite similarities.
Data science education
Following the popularity of widespread data science and the interest therein, many colleges and universities launched data science curriculum throughout the 2010s to keep up with this new demand. Tooling at the time was relatively primitive compared to today’s standards, and included statistical models in Python or R which had to be carefully selected by and tuned by trained professionals which was an art form in-and-of itself, and results interpreted to maximize for different scoring methods. Early educational programs focused on programming and statistics as core curriculum to produce data scientists, as model training and interpretation were necessary skills prior to modern-day tooling. As time progressed, so did the industry. The core statistics were abstracted away from the data scientist, where instead the model could be instructed to optimize for different scoring methods. The code required to train models and run inference was reduced until nearly completely abstracted. Many modern data science programs being taught in universities today are based on outdated skills brought forth by the popularity of early data science, and have yet to adapt.
What are data scientists doing today?
This varies wildly, as is common with such technological revolution. Many long-time Data Scientists still believe in hand-building models and hand-tuning models. Many others prefer to use sophisticated tooling to increase the accuracy of their model and ultimately implement a greater number of production-ready models. As described in the aforementioned HBR article, the definition of Data Scientist varies drastically. Some companies rely on Data Scientists to design tests to monitor metrics and make decisions, using relatively few statistical principles. Other companies rely on Data Scientists to design new statistical models, or refine existing statistical models to increase performance of their products or predictive pipelines. Still more, there are many companies today who rely on Data Scientists to perform many of the same functions as the modern Analyst; writing SQL queries to manipulate data, and deliver the data to the business in the form of spreadsheets or data visualizations. With such a wide spread of responsibilities, much confusion exists in the market, and many differing definitions are commonly accepted.
Why is AutoML gaining traction?
For full disclosure, I classify myself as a Data Engineer, not Data Scientist. I have built data pipelines to support predictive modeling, but I have no interest in building or tuning models. I was introduced to AutoML in 2021, in the form of Google’s Vertex AI. [As a disclosure, I hold no stock in Alphabet, Inc. and don’t stand to profit in any way.] AutoML products like Vertex allow individuals without formal knowledge of training models and tuning hyper parameters to produce powerful statistical predictions. The modern Data Analyst gets access to new capabilities for the first time in history. Analysts are typically at the forefront of understanding business problems and how to apply data to solving them. While historically, analysts could only be reactionary and display past trends, analysts can now build features to be ingested by AutoML tooling to produce insight into the future.
Will Analyst be the next Data Scientist? My opinion is that both titles will exist with much ambiguity for years to come. Data Scientist may be a logical progress for the Analyst, once the Analyst has mastered the usage of the business data, and learned how to work with AutoML tooling. However, with this new tooling, easy to learn skills such as feature engineering can be taught on the job, rather than requiring lengthy, often outdated, and costly degrees.
Who is designing the next generation of predictive modeling?
For the vast majority, companies such as Google and Meta are employing teams of specialized PhD Research Scientists to refine their most valuable predictive models. These Research Scientists are highly specialized in programming and statistics, and understand deep emerging fields of mathematics and are able to apply these complex concepts to improving the available tooling.
Significant discoveries are still being made every day by teams of Research Scientists. GPT-3 or “Generative Pre-trained Transformer version 3” is a sophistical natural language model trained on billions of samples, capable of writing entire articles, having human-like conversations, and answering questions with a surprising degree of accuracy. Computer vision models, such as those found in PyTorch and TensorFlow are capable of object-detection, segmentation, and classification, at levels that rival those of humans — except in the case of Chihuahuas and muffins.
Who is the Data Scientist of the future?
With Google’s investment in industry-leading products such as Vertex AI, I believe Google has demonstrated a realization in the value of coupling sophisticated Auto ML products with domain knowledge experts, and abstracting away much of the programming and statistics required by the Data Scientist of yesterday.
I believe that domain knowledge will rule the future. Understanding the relationship between inputs and outputs in human-interpretable ways, and having the skills to communicate this knowledge is the most important input to predictive modeling. I wouldn’t be surprised if MBA programs began requiring SQL courses, or taught feature engineering. The focus has shifted from model tuning to model input, as the single biggest way to increase model accuracy. To produce better model input, the business problem that the model is being applied to must be understood by specialists. These specialists are likely to be called Data Scientists, but share very little DNA with early Data Scientists. I believe the Data Scientist of the future will spend the majority of their time designing experiments, confirming hypothesis, embedding themselves close to the business, and writing SQL to build features to improve model accuracy.
Deep statisticians will be able to focus their specialized skills on improving the accuracy of automated machine learning architectures, equipping these futuristic Data Scientists with ever-improving tools to produce ever-higher accuracy.
About the author: I am a vendor at Google, not a full-time employee, working for a single Data Engineering team of Alphabet, inc. for over 2 years. Prior to my time delivering services for Google, I have spent 6 years combined in the healthcare and technology sectors. My opinions are strictly personal observations, are not the opinions of any corporation, are not excerpted from any proprietary material, and reveal no trade secrets.